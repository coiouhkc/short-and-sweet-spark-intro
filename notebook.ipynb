{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce80c8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import min, lit, lower, trim, split, sum, col, desc, when, date_format, log, log10, element_at, regexp_replace, collect_list\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_aliases = StructType([\n",
    "    StructField('email',\n",
    "                StringType(), True),\n",
    "    StructField('alias',\n",
    "                StringType(), True)\n",
    "])\n",
    "\n",
    "aliases_df = spark.read.schema(schema_aliases).csv(\"emails/aliases.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('./commits/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('commit-hash',\n",
    "                StringType(), True),\n",
    "    StructField('commit-authored-datetime',\n",
    "                TimestampType(), True),\n",
    "    StructField('commit-author-email',\n",
    "                StringType(), True),\n",
    "    StructField('commit-committed-datetime',\n",
    "                TimestampType(), True),\n",
    "    StructField('commit-committer-email',\n",
    "                StringType(), True),\n",
    "    StructField('file-name',\n",
    "                StringType(), True),\n",
    "    StructField('file-insertions',\n",
    "                DoubleType(), True),\n",
    "    StructField('file-deletions',\n",
    "                DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ac0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = None\n",
    "\n",
    "for file in files:\n",
    "    df = spark.read.schema(schema).csv(\"commits/\" + file, header=True)\n",
    "    df = df.withColumn(\"source\", lit(file.split('.')[0]))\n",
    "\n",
    "    if combined_df is None:\n",
    "        combined_df = df\n",
    "    else:\n",
    "        combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df = combined_df \\\n",
    "    .join(aliases_df.alias(\"a1\"), trim(lower(col(\"commit-committer-email\"))) == col(\"a1.email\"), \"left\") \\\n",
    "    .join(aliases_df.alias(\"a2\"), trim(lower(col(\"commit-author-email\"))) == col(\"a2.email\"), \"left\") \\\n",
    "    .select(combined_df[\"*\"], col(\"a1.alias\").alias(\"committer\"), col(\"a2.alias\").alias(\"author\")) \\\n",
    "    .drop(col(\"commit-committer-email\")) \\\n",
    "    .drop(col(\"commit-author-email\")) \\\n",
    "    .filter(col(\"committer\").isNotNull()) \\\n",
    "    .filter(col(\"author\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41e8ef",
   "metadata": {},
   "source": [
    "# Use case 0\n",
    "## Commit size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6178fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human = combined_df.filter((col('file-deletions') < 100000) & (col('file-insertions') < 100000))\n",
    "insertions = df_human.select(col('file-insertions')).toPandas()\n",
    "deletions = df_human.select(col('file-deletions')).toPandas()\n",
    "\n",
    "plt.scatter(insertions, deletions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1af5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = combined_df \\\n",
    "    .withColumn('file-changes', col('file-insertions') + col('file-deletions')) \\\n",
    "    .select(col('file-changes')) \\\n",
    "    .filter(col('file-changes') < 100000) \\\n",
    "    .toPandas()\n",
    "\n",
    "plt.hist(changes, bins=100, color='skyblue', edgecolor='black', log=True)\n",
    "\n",
    "changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessaries libraries\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "# Set notebook mode to work in offline\n",
    "pyo.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df \\\n",
    "    .withColumn('file-changes', log10(col('file-insertions') + col('file-deletions'))) \\\n",
    "    .select(col('file-changes')) \\\n",
    "    .filter(col('file-changes') < 100000) \\\n",
    "    .plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05cbbf",
   "metadata": {},
   "source": [
    "# Use case 1\n",
    "## Commits per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_per_month_df = combined_df \\\n",
    "    .select(col(\"commit-committed-datetime\")) \\\n",
    "    .withColumn(\"commit-committed-month\", date_format(col(\"commit-committed-datetime\"), \"yyyy-MM\")) \\\n",
    "    .groupBy(col(\"commit-committed-month\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"commit-committed-month\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b836e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessaries libraries\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "# Set notebook mode to work in offline\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "commits_per_month_df.plot.bar(y=\"count\", x=\"commit-committed-month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e4ccc",
   "metadata": {},
   "source": [
    "# Use case 2\n",
    "## Most productive committers all time/ per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .filter(date_format(col(\"commit-committed-datetime\"), \"yyyy-MM\") == \"2025-01\") \\\n",
    "\n",
    "consolidated_df \\\n",
    "    .select(col(\"commit-committed-datetime\"), col(\"committer\")) \\\n",
    "    .groupBy(col(\"committer\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .plot.bar(y=\"count\", x=\"committer\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df \\\n",
    "    .select(col(\"commit-committed-datetime\"), col(\"committer\")) \\\n",
    "    .filter(date_format(col(\"commit-committed-datetime\"), \"yyyy\") == \"2025\") \\\n",
    "    .groupBy(col(\"committer\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .plot.bar(y=\"count\", x=\"committer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34a9f6",
   "metadata": {},
   "source": [
    "# Use case 3\n",
    "## LOCs per languages per repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ab065",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"<repo>\"\n",
    "\n",
    "consolidated_df \\\n",
    "    .select(col(\"source\"), col(\"file-name\"), col(\"file-insertions\"), col(\"file-deletions\")) \\\n",
    "    .filter(col(\"source\") == source) \\\n",
    "    .withColumn(\"file-lines\", col(\"file-insertions\") - col(\"file-deletions\")) \\\n",
    "    .withColumn(\"lang-arr\",  split(col(\"file-name\"), \"\\\\.\")) \\\n",
    "    .withColumn(\"lang\", regexp_replace(lower(element_at(col(\"lang-arr\"), -1)), \"[^\\\\w]\", \"\")) \\\n",
    "    .groupBy(col(\"lang\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf568c0",
   "metadata": {},
   "source": [
    "# Use case 4\n",
    "## ML\n",
    "\n",
    "Classification problem?\n",
    "\n",
    "Given changed files and the amount of changes, try to predict the most likely author alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different files in repo?\n",
    "\n",
    "source = \"<repo>\"\n",
    "\n",
    "consolidated_df \\\n",
    "    .filter(col(\"source\") == source) \\\n",
    "    .select(col(\"file-name\")) \\\n",
    "    .distinct() \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4452e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e28da0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data = consolidated_df \\\n",
    "    .filter(col(\"source\") == source) \\\n",
    "    .select(col(\"commit-hash\"), col(\"file-name\"), col(\"committer\")) \\\n",
    "    .groupBy(col(\"commit-hash\")) \\\n",
    "    .agg(collect_list(\"file-name\").alias(\"files\"), min(\"committer\").alias(\"committer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3dfc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"files\", outputCol=\"features\")\n",
    "\n",
    "model = cv.fit(data)\n",
    "\n",
    "result = model.transform(data).select(col(\"commit-hash\"), col(\"features\"), col(\"committer\"))\n",
    "\n",
    "si = StringIndexer(inputCol=\"committer\", outputCol=\"committer_i\").fit(result)\n",
    "\n",
    "its = IndexToString(inputCol=\"prediction\", outputCol=\"predicted\", labels=si.labels)\n",
    "\n",
    "#.sample(fraction=.1) \\\n",
    "trainDF, testDF = \\\n",
    "    result \\\n",
    "        .randomSplit([.8, .2], seed=42)\n",
    "\n",
    "lr = RandomForestClassifier(featuresCol = 'features', labelCol = 'committer_i')\n",
    "\n",
    "pipeline = Pipeline(stages = [si, lr, its])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"committer\", \"predicted\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e016a42",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"committer_i\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predDF)\n",
    "print(f\"ACCURACY: {accuracy}\")\n",
    "\n",
    "evaluator.setMetricName(\"logLoss\")\n",
    "logLoss = evaluator.evaluate(predDF)\n",
    "print(f\"LOGLOSS: {logLoss}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
